{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-러닝 이해하기: 완전 가이드\n",
    "\n",
    "# 목차\n",
    "\n",
    "- [소개](#introduction)\n",
    "- [Q-러닝이란?](#what-is-q-learning)\n",
    "- [Q-러닝 사용처 및 방법](#where-and-how-q-learning-is-used)\n",
    "- [Q-러닝의 수학적 기초](#mathematical-foundation-of-q-learning)\n",
    "  - [복잡한 원본 버전](#complex-original-version)\n",
    "  - [단순화된 버전](#simplified-version)\n",
    "- [Q-러닝 단계별 설명](#step-by-step-explanation-of-q-learning)\n",
    "- [Q-러닝의 핵심 요소](#key-components-of-q-learning)\n",
    "  - [Q-테이블](#q-table)\n",
    "  - [탐험 vs. 활용](#exploration-vs-exploitation)\n",
    "  - [학습률 (α)](#learning-rate-a)\n",
    "  - [할인율 (γ)](#discount-factor-g)\n",
    "- [실용 예제: 그리드 월드](#practical-example-grid-world)\n",
    "  - [환경 설정](#setting-up-the-environment)\n",
    "  - [간단한 환경 생성](#creating-a-simple-environment)\n",
    "  - [Q-러닝 알고리즘 구현](#implementing-the-q-learning-algorithm)\n",
    "  - [Q-테이블 초기화 및 업데이트](#q-table-initialization-and-updates)\n",
    "  - [탐험 vs. 활용 전략](#exploration-vs-exploitation-strategy)\n",
    "  - [Q-러닝 알고리즘 실행](#running-the-q-learning-algorithm)\n",
    "  - [학습 과정 시각화](#visualizing-the-learning-process)\n",
    "  - [Q-값 및 최적 정책 분석](#analyzing-q-values-and-optimal-policy)\n",
    "- [다른 하이퍼파라미터로 테스트하기 (선택 사항)](#testing-with-different-hyperparameters-optional)\n",
    "- [다른 환경에 Q-러닝 적용하기 (절벽 걷기)](#applying-q-learning-to-different-environments-cliff-walking)\n",
    "- [일반적인 문제점 및 해결책](#common-challenges-and-solutions)\n",
    "- [Q-러닝 vs. 다른 강화 학습 알고리즘](#q-learning-vs-other-reinforcement-learning-algorithms)\n",
    "  - [Q-러닝의 장점](#advantages-of-q-learning)\n",
    "  - [Q-러닝의 한계](#limitations-of-q-learning)\n",
    "  - [관련 알고리즘](#related-algorithms)\n",
    "- [결론](#conclusion)\n",
    "\n",
    "## Q-러닝이란?\n",
    "\n",
    "Q-러닝은 강화 학습 알고리즘으로, 에이전트가 시행착오를 통해 어떤 행동이 가장 높은 보상을 얻는지 발견함으로써 주어진 상황에서 최적의 행동을 학습할 수 있게 합니다. 이는 모델-프리(model-free), 가치 기반(value-based) 학습 알고리즘으로, 환경의 모델 없이도 학습할 수 있음을 의미합니다. 대신, 다른 상태에서 행동을 취한 후 받는 보상으로부터 학습합니다.\n",
    "\n",
    "Q-러닝의 \"Q\"는 \"품질(quality)\"을 의미하며, 본질적으로 주어진 행동이 미래의 보상을 얻는 데 얼마나 유용한지를 나타냅니다.\n",
    "\n",
    "## Q-러닝 사용처 및 방법\n",
    "\n",
    "Q-러닝은 다음과 같은 분야에서 널리 사용됩니다:\n",
    "\n",
    "1. **로봇 공학**: 로봇에게 환경 탐색, 물체 집기, 작업 완료 방법 교육\n",
    "2. **게임 플레이**: 게임(아타리 게임 또는 보드 게임 등)을 마스터할 수 있는 에이전트 생성\n",
    "3. **자원 관리**: 엘리베이터 제어 또는 신호등 관리와 같은 시스템에서의 의사결정 최적화\n",
    "4. **추천 시스템**: 사용자 선호도를 학습하여 제품 또는 콘텐츠 추천\n",
    "5. **자율 주행 차량**: 자율 주행차가 의사결정을 내리는 데 도움\n",
    "\n",
    "Q-러닝은 다음과 같은 환경에서 특히 잘 작동합니다:\n",
    "- 환경의 규칙이나 모델을 모를 때\n",
    "- 환경에 명확한 상태와 행동이 있을 때\n",
    "- 잘 정의된 보상 신호가 있을 때\n",
    "- 환경이 완전히 관찰 가능할 때 (에이전트가 전체 상태를 볼 수 있음)\n",
    "\n",
    "## Q-러닝의 수학적 기초\n",
    "\n",
    "### 복잡한 원본 버전\n",
    "\n",
    "Q-러닝 알고리즘은 벨만 방정식을 사용하여 Q-값을 업데이트합니다:\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)\\right]$$\n",
    "\n",
    "여기서:\n",
    "- $Q(s_t, a_t)$는 상태 $s_t$와 행동 $a_t$에 대한 Q-값입니다.\n",
    "- $\\alpha$는 학습률입니다 (0 < $\\alpha$ ≤ 1).\n",
    "- $r_t$는 상태 $s_t$에서 행동 $a_t$를 취한 후 받은 보상입니다.\n",
    "- $\\gamma$는 미래 보상에 대한 할인율입니다 (0 ≤ $\\gamma$ ≤ 1).\n",
    "- $\\max_{a} Q(s_{t+1}, a)$는 다음 상태 $s_{t+1}$에서 가능한 모든 행동에 대한 최대 Q-값입니다.\n",
    "- 대괄호 안의 항 $[r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]$은 시간차(Temporal Difference, TD) 오차입니다.\n",
    "\n",
    "### 단순화된 버전\n",
    "\n",
    "더 간단한 용어로 Q-러닝 업데이트는 다음과 같이 이해할 수 있습니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_{\\text{new}} = Q_{\\text{old}} + \\alpha \\left[ R + \\gamma \\max Q_{\\text{future}} - Q_{\\text{old}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "또는 더 간단하게:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_{\\text{new}} = Q_{\\text{old}} + \\alpha \\left[ \\text{Target} - Q_{\\text{old}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "여기서 \"Target\"은 보상과 할인된 미래 가치의 합입니다.\n",
    "\n",
    "## Q-러닝 단계별 설명\n",
    "\n",
    "1. **Q-테이블 초기화**: 각 상태에 대한 행과 각 행동에 대한 열을 가진 테이블을 생성하고, 초기에는 0 또는 임의의 값으로 채웁니다.\n",
    "\n",
    "2. **행동 선택**: 상태 $s$에서 탐험 전략(예: 엡실론 그리디)을 사용하여 행동 $a$를 선택합니다.\n",
    "\n",
    "3. **행동 수행**: 행동 $a$를 실행하고, 보상 $r$과 새로운 상태 $s'$를 관찰합니다.\n",
    "\n",
    "4. **Q-값 업데이트**: Q-러닝 업데이트 공식을 적용하여 상태-행동 쌍의 Q-값을 조정합니다.\n",
    "\n",
    "5. **다음 상태로 이동**: 현재 상태를 새로운 상태 $s'$로 설정합니다.\n",
    "\n",
    "6. **2-5단계 반복**: 종료 상태에 도달하거나 최대 단계 수에 도달할 때까지 이 과정을 계속합니다.\n",
    "\n",
    "7. **여러 에피소드 반복**: 에이전트가 다른 상태-행동 쌍을 탐험하고 Q-값을 개선할 수 있도록 여러 에피소드를 실행합니다.\n",
    "\n",
    "## Q-러닝의 핵심 요소\n",
    "\n",
    "### Q-테이블\n",
    "Q-테이블은 다음과 같은 조회 테이블입니다:\n",
    "- 행은 환경의 상태를 나타냅니다.\n",
    "- 열은 가능한 행동을 나타냅니다.\n",
    "- 각 셀에는 해당 상태에서 해당 행동을 취했을 때 예상되는 미래 보상을 나타내는 Q-값이 포함됩니다.\n",
    "\n",
    "예를 들어, 간단한 그리드 월드에서:\n",
    "\n",
    "| 상태  | 위  | 아래 | 왼쪽 | 오른쪽 |\n",
    "|-------|---|------|------|-------|\n",
    "| (0,0) | 0.0 | 0.0 | 0.0 | 0.0 |\n",
    "| (0,1) | 0.0 | 0.0 | 0.0 | 0.0 |\n",
    "| ... | ... | ... | ... | ... |\n",
    "\n",
    "### 탐험 vs. 활용\n",
    "\n",
    "강화 학습의 핵심 과제 중 하나는 다음 사이의 균형입니다:\n",
    "\n",
    "- **탐험(Exploration)**: 더 나은 보상을 발견하기 위해 새로운 행동을 시도하는 것 (위험 감수)\n",
    "- **활용(Exploitation)**: 에이전트가 이미 알고 있는 것을 사용하여 보상을 최대화하는 것 (안전하게 플레이)\n",
    "\n",
    "**엡실론 그리디(epsilon-greedy)** 전략은 이러한 균형을 맞추기 위해 일반적으로 사용됩니다:\n",
    "- 확률 $\\epsilon$으로 임의의 행동을 선택합니다 (탐험).\n",
    "- 확률 $1-\\epsilon$으로 가장 높은 Q-값을 가진 행동을 선택합니다 (활용).\n",
    "- $\\epsilon$은 일반적으로 에이전트가 환경에 대해 더 많이 배우면서 시간이 지남에 따라 감소합니다.\n",
    "\n",
    "### 학습률 (α)\n",
    "- 새로운 정보가 오래된 정보를 얼마나 덮어쓰는지 제어합니다.\n",
    "- 높은 학습률 (1에 가까움): 빠르게 학습하지만 불안정해질 수 있습니다.\n",
    "- 낮은 학습률 (0에 가까움): 느리게 학습하지만 더 안정적입니다.\n",
    "- 일반적인 값: 0.1 ~ 0.5\n",
    "\n",
    "### 할인율 (γ)\n",
    "- 즉각적인 보상과 비교하여 미래 보상의 중요성을 결정합니다.\n",
    "- γ = 0: 에이전트는 즉각적인 보상만 고려합니다 (단기적).\n",
    "- γ = 1: 에이전트는 미래 보상을 즉각적인 보상과 동일하게 평가합니다 (장기적).\n",
    "- 일반적인 값: 0.9 ~ 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실용 예제: 그리드 월드\n",
    "\n",
    "노트북의 예제에서는 Q-러닝이 다음과 같은 그리드 월드에 적용됩니다:\n",
    "- 에이전트는 4×4 그리드를 탐색합니다.\n",
    "- 종료 상태는 (0,0) (보상 1)과 (3,3) (보상 10)입니다.\n",
    "- 다른 모든 상태는 보상 0입니다.\n",
    "- 에이전트는 위, 아래, 왼쪽, 오른쪽으로 이동할 수 있습니다.\n",
    "- 목표는 가장 높은 보상으로 가는 최적의 경로를 학습하는 것입니다.\n",
    "\n",
    "에이전트가 탐험하면서 Q-값을 업데이트하고 점차 환경을 통과하는 최상의 경로를 학습합니다. 시각화는 에이전트가 무작위로 탐험을 시작하여 결국 최적 정책으로 수렴하는 방식을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정\n",
    "수치 연산을 위한 NumPy와 시각화를 위한 Matplotlib 등 필요한 라이브러리를 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import numpy as np  # 수치 연산용\n",
    "import matplotlib.pyplot as plt  # 시각화용\n",
    "\n",
    "# 타입 힌트 임포트\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# 재현성을 위한 시드 설정\n",
    "np.random.seed(42)\n",
    "\n",
    "# Jupyter Notebook에서 인라인 플로팅 활성화\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 환경 생성\n",
    "\n",
    "Q-러닝 알고리즘을 위한 간단한 환경을 만들기 위해 4x4 그리드 월드를 정의합니다. 그리드 월드는 다음과 같은 속성을 갖습니다:\n",
    "    \n",
    "- 4개의 행과 4개의 열\n",
    "- 가능한 행동: 'up', 'down', 'left', 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드 월드 환경 정의\n",
    "def create_gridworld(\n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    terminal_states: List[Tuple[int, int]], \n",
    "    rewards: Dict[Tuple[int, int], int]\n",
    ") -> Tuple[np.ndarray, List[Tuple[int, int]], List[str]]:\n",
    "    \"\"\"\n",
    "    간단한 그리드 월드 환경을 생성합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - rows (int): 그리드의 행 수.\n",
    "    - cols (int): 그리드의 열 수.\n",
    "    - terminal_states (List[Tuple[int, int]]): (행, 열) 튜플로 구성된 종료 상태 목록.\n",
    "    - rewards (Dict[Tuple[int, int], int]): (행, 열)을 보상 값에 매핑하는 딕셔너리.\n",
    "    \n",
    "    반환값:\n",
    "    - grid (np.ndarray): 보상을 포함한 그리드를 나타내는 2D 배열.\n",
    "    - state_space (List[Tuple[int, int]]): 그리드 내 가능한 모든 상태의 목록.\n",
    "    - action_space (List[str]): 가능한 행동 목록 ('up', 'down', 'left', 'right').\n",
    "    \"\"\"\n",
    "    # 그리드를 0으로 초기화\n",
    "    grid = np.zeros((rows, cols))\n",
    "    \n",
    "    # 지정된 상태에 보상 할당\n",
    "    for (row, col), reward in rewards.items():\n",
    "        grid[row, col] = reward\n",
    "    \n",
    "    # 상태 공간을 모든 가능한 (행, 열) 쌍으로 정의\n",
    "    state_space = [\n",
    "        (row, col) \n",
    "        for row in range(rows) \n",
    "        for col in range(cols)\n",
    "    ]\n",
    "    \n",
    "    # 행동 공간을 네 가지 가능한 이동으로 정의\n",
    "    action_space = ['up', 'down', 'left', 'right']\n",
    "    \n",
    "    return grid, state_space, action_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 상태 전이 함수가 필요합니다. 이 함수는 현재 상태와 행동을 입력으로 받아 다음 상태를 반환합니다. 이는 에이전트가 취하는 행동에 따라 그리드를 이동하는 것으로 생각할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 전이 함수 정의\n",
    "def state_transition(state: Tuple[int, int], action: str, rows: int, cols: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    현재 상태와 행동이 주어졌을 때 다음 상태를 계산합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - state (Tuple[int, int]): 현재 상태 (행, 열).\n",
    "    - action (str): 취할 행동 ('up', 'down', 'left', 'right').\n",
    "    - rows (int): 그리드의 행 수.\n",
    "    - cols (int): 그리드의 열 수.\n",
    "    \n",
    "    반환값:\n",
    "    - Tuple[int, int]: 행동을 취한 후의 결과 상태 (행, 열).\n",
    "    \"\"\"\n",
    "    # 현재 상태를 행과 열로 언패킹\n",
    "    row, col = state\n",
    "\n",
    "    # 행동에 따라 행 또는 열 업데이트, 경계를 존중하도록 보장\n",
    "    if action == 'up' and row > 0:  # 가장 위쪽 행이 아니면 위로 이동\n",
    "        row -= 1\n",
    "    elif action == 'down' and row < rows - 1:  # 가장 아래쪽 행이 아니면 아래로 이동\n",
    "        row += 1\n",
    "    elif action == 'left' and col > 0:  # 가장 왼쪽 열이 아니면 왼쪽으로 이동\n",
    "        col -= 1\n",
    "    elif action == 'right' and col < cols - 1:  # 가장 오른쪽 열이 아니면 오른쪽으로 이동\n",
    "        col += 1\n",
    "\n",
    "    # 새로운 상태를 튜플로 반환\n",
    "    return (row, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 에이전트가 환경과 상호작용할 수 있으므로 보상 함수를 정의해야 합니다. 이 함수는 주어진 상태에 대한 보상을 반환하며, 이는 훈련 중 Q-값을 업데이트하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보상 함수 정의\n",
    "def get_reward(state: Tuple[int, int], rewards: Dict[Tuple[int, int], int]) -> int:\n",
    "    \"\"\"\n",
    "    주어진 상태에 대한 보상을 얻습니다.\n",
    "\n",
    "    매개변수:\n",
    "    - state (Tuple[int, int]): 현재 상태 (행, 열).\n",
    "    - rewards (Dict[Tuple[int, int], int]): (행, 열)을 보상 값에 매핑하는 딕셔너리.\n",
    "\n",
    "    반환값:\n",
    "    - int: 주어진 상태에 대한 보상. 상태가 rewards 딕셔너리에 없으면 0을 반환합니다.\n",
    "    \"\"\"\n",
    "    # rewards 딕셔너리를 사용하여 주어진 상태의 보상을 가져옵니다.\n",
    "    # 상태를 찾을 수 없으면 기본 보상 0을 반환합니다.\n",
    "    return rewards.get(state, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그리드 월드 환경과 필요한 도우미 함수를 정의했으므로 간단한 예제로 테스트해 보겠습니다. (0, 0)과 (3, 3)에 각각 1과 10의 보상을 갖는 두 개의 종료 상태가 있는 4x4 그리드를 생성합니다. 그런 다음 상태 (2, 2)에서 위쪽으로 이동하여 상태 전이 및 보상 함수를 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드 월드 환경 사용 예제\n",
    "\n",
    "# 그리드 차원(4x4), 종료 상태 및 보상 정의\n",
    "rows, cols = 4, 4  # 그리드의 행과 열 수\n",
    "terminal_states = [(0, 0), (3, 3)]  # 보상이 있는 종료 상태\n",
    "rewards = {(0, 0): 1, (3, 3): 10}  # 종료 상태에 대한 보상\n",
    "\n",
    "# 그리드 월드 환경 생성\n",
    "grid, state_space, action_space = create_gridworld(rows, cols, terminal_states, rewards)\n",
    "\n",
    "# 상태 전이 및 보상 함수 테스트\n",
    "current_state = (2, 2)  # 시작 상태\n",
    "action = 'up'  # 취할 행동\n",
    "next_state = state_transition(current_state, action, rows, cols)  # 다음 상태 계산\n",
    "reward = get_reward(next_state, rewards)  # 다음 상태에 대한 보상 얻기\n",
    "\n",
    "# 결과 출력\n",
    "print(\"그리드 월드:\")  # 보상이 포함된 그리드 표시\n",
    "print(grid)\n",
    "print(f\"현재 상태: {current_state}\")  # 현재 상태 표시\n",
    "print(f\"취한 행동: {action}\")  # 취한 행동 표시\n",
    "print(f\"다음 상태: {next_state}\")  # 결과 다음 상태 표시\n",
    "print(f\"보상: {reward}\")  # 다음 상태에 대한 보상 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지정된 차원, 종료 상태 및 보상으로 그리드 월드 환경이 생성된 것을 볼 수 있습니다. 시작 상태 (2, 2)와 취할 행동 ('up')을 무작위로 선택했습니다. 다음 상태는 (1, 2)로 계산되며 보상은 0입니다. 왜냐하면 `terminal_states` 딕셔너리에 이 상태가 포함되어 있지 않기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-러닝 알고리즘 구현\n",
    "\n",
    "이제 상태 전이 및 보상 함수를 사용하여 그리드 월드 환경을 성공적으로 구현했습니다. 이 환경을 사용하여 Q-러닝 알고리즘을 구현할 수 있습니다. 먼저 Q-테이블을 초기화해야 합니다. Q-테이블은 상태-행동 쌍을 Q-값에 매핑하는 딕셔너리입니다. Q-값은 주어진 상태에서 특정 행동을 취할 때 예상되는 누적 보상을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블 초기화\n",
    "def initialize_q_table(state_space: List[Tuple[int, int]], action_space: List[str]) -> Dict[Tuple[int, int], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    모든 상태-행동 쌍에 대해 Q-테이블을 0으로 초기화합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - state_space (List[Tuple[int, int]]): 환경의 모든 가능한 상태 목록, (행, 열) 튜플로 표현됩니다.\n",
    "    - action_space (List[str]): 가능한 모든 행동 목록 (예: 'up', 'down', 'left', 'right').\n",
    "\n",
    "    반환값:\n",
    "    - q_table (Dict[Tuple[int, int], Dict[str, float]]): 각 상태가 다른 딕셔너리에 매핑되는 딕셔너리입니다.\n",
    "      내부 딕셔너리는 각 행동을 해당 Q-값에 매핑하며, 0으로 초기화됩니다.\n",
    "    \"\"\"\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "    for state in state_space:\n",
    "        # 주어진 상태의 모든 행동에 대한 Q-값을 0으로 초기화\n",
    "        q_table[state] = {action: 0.0 for action in action_space}\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 엡실론 그리디 정책을 사용하여 행동을 선택하는 함수를 정의합니다. 만약 무작위 값이 엡실론보다 작으면 무작위 행동을 선택하고, 그렇지 않으면 현재 상태에 대해 가장 높은 Q-값을 가진 행동을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엡실론 그리디 정책을 사용하여 행동 선택\n",
    "def choose_action(state: Tuple[int, int], q_table: Dict[Tuple[int, int], Dict[str, float]], action_space: List[str], epsilon: float) -> str:\n",
    "    \"\"\"\n",
    "    엡실론 그리디 정책을 사용하여 행동을 선택합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - state (Tuple[int, int]): 현재 상태 (행, 열).\n",
    "    - q_table (Dict[Tuple[int, int], Dict[str, float]]): 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - action_space (List[str]): 가능한 행동 목록 (예: 'up', 'down', 'left', 'right').\n",
    "    - epsilon (float): 탐험 비율 (0 <= epsilon <= 1).\n",
    "\n",
    "    반환값:\n",
    "    - str: 선택된 행동.\n",
    "    \"\"\"\n",
    "    # 확률 엡실론으로 무작위 행동 선택 (탐험)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(action_space)\n",
    "    # 그렇지 않으면 현재 상태에 대해 가장 높은 Q-값을 가진 행동 선택 (활용)\n",
    "    else:\n",
    "        # q_table[state] 딕셔너리에서 값이 가장 큰 키(행동)를 반환\n",
    "        return max(q_table[state], key=q_table[state].get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행동을 취하고 보상과 다음 상태를 관찰한 후에는 Q-러닝 업데이트 규칙을 사용하여 Q-값을 업데이트할 수 있습니다. 업데이트 규칙은 다음과 같습니다:\n",
    "$$\n",
    "Q(s, a) = Q(s, a) + α * [R(s) + γ * max(Q(s', a')) - Q(s, a)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-값 업데이트\n",
    "def update_q_value(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    state: Tuple[int, int], \n",
    "    action: str, \n",
    "    reward: int, \n",
    "    next_state: Tuple[int, int], \n",
    "    alpha: float, \n",
    "    gamma: float, \n",
    "    action_space: List[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Q-러닝 업데이트 규칙을 사용하여 Q-값을 업데이트합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - q_table (Dict[Tuple[int, int], Dict[str, float]]): 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - state (Tuple[int, int]): 현재 상태 (행, 열).\n",
    "    - action (str): 취한 행동.\n",
    "    - reward (int): 받은 보상.\n",
    "    - next_state (Tuple[int, int]): 다음 상태 (행, 열).\n",
    "    - alpha (float): 학습률 (0 < alpha <= 1).\n",
    "    - gamma (float): 할인율 (0 <= gamma <= 1).\n",
    "    - action_space (List[str]): 가능한 행동 목록.\n",
    "\n",
    "    반환값:\n",
    "    - None: Q-테이블을 제자리에서 업데이트합니다.\n",
    "    \"\"\"\n",
    "    # 다음 상태에 대해 가능한 모든 행동 중 최대 Q-값을 가져옵니다.\n",
    "    # 다음 상태가 q_table에 있으면 해당 상태의 값들 중 최대값을, 없으면 0.0을 사용합니다.\n",
    "    max_next_q: float = max(q_table[next_state].values()) if next_state in q_table else 0.0\n",
    "\n",
    "    # 현재 상태-행동 쌍의 Q-값을 Q-러닝 공식으로 업데이트합니다.\n",
    "    q_table[state][action] += alpha * (reward + gamma * max_next_q - q_table[state][action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 그리드 월드 환경, 상태 전이 함수, 보상 함수 및 Q-러닝 업데이트 규칙을 정의했습니다. 또한 Q-테이블 초기화, 엡실론 그리디 정책을 사용한 행동 선택 및 Q-값 업데이트 함수를 구현했습니다. 이제 모든 것을 종합하여 그리드 월드 환경에서 여러 에피소드의 Q-러닝을 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 에피소드 실행\n",
    "def run_episode(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    state_space: List[Tuple[int, int]], \n",
    "    action_space: List[str], \n",
    "    rewards: Dict[Tuple[int, int], int], \n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    alpha: float, \n",
    "    gamma: float, \n",
    "    epsilon: float, \n",
    "    max_steps: int\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Q-러닝의 단일 에피소드를 실행합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - q_table (Dict[Tuple[int, int], Dict[str, float]]): 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - state_space (List[Tuple[int, int]]): 환경의 모든 가능한 상태 목록.\n",
    "    - action_space (List[str]): 가능한 행동 목록 (예: 'up', 'down', 'left', 'right').\n",
    "    - rewards (Dict[Tuple[int, int], int]): 상태(행, 열)를 보상 값에 매핑하는 딕셔너리.\n",
    "    - rows (int): 그리드의 행 수.\n",
    "    - cols (int): 그리드의 열 수.\n",
    "    - alpha (float): 학습률 (0 < alpha <= 1).\n",
    "    - gamma (float): 할인율 (0 <= gamma <= 1).\n",
    "    - epsilon (float): 탐험 비율 (0 <= epsilon <= 1).\n",
    "    - max_steps (int): 에피소드에서 허용되는 최대 단계 수.\n",
    "\n",
    "    반환값:\n",
    "    - int: 에피소드 동안 누적된 총 보상.\n",
    "    \"\"\"\n",
    "    # 무작위 상태에서 시작\n",
    "    state: Tuple[int, int] = state_space[np.random.choice(len(state_space))]\n",
    "    total_reward: int = 0  # 에피소드의 총 보상 초기화\n",
    "\n",
    "    # 최대 단계 수만큼 반복\n",
    "    for _ in range(max_steps):\n",
    "        # 엡실론 그리디 정책을 사용하여 행동 선택\n",
    "        action: str = choose_action(state, q_table, action_space, epsilon)\n",
    "        \n",
    "        # 선택된 행동에 따라 다음 상태 계산\n",
    "        next_state: Tuple[int, int] = state_transition(state, action, rows, cols)\n",
    "        \n",
    "        # 다음 상태에 대한 보상 얻기\n",
    "        reward: int = get_reward(next_state, rewards)\n",
    "        \n",
    "        # 현재 상태-행동 쌍의 Q-값 업데이트\n",
    "        update_q_value(q_table, state, action, reward, next_state, alpha, gamma, action_space)\n",
    "        \n",
    "        # 보상 누적\n",
    "        total_reward += reward\n",
    "        \n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "        \n",
    "        # 에이전트가 종료 상태에 도달했는지 확인\n",
    "        if state in terminal_states:\n",
    "            break\n",
    "    \n",
    "    # 에피소드 동안 누적된 총 보상 반환\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그리드 월드 환경에서 에이전트를 훈련하기 위해 여러 에피소드의 Q-러닝을 실행할 수 있습니다. 각 에피소드에서 누적된 총 보상을 추적하고 라인 플롯을 사용하여 시간 경과에 따른 보상을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-러닝 알고리즘의 하이퍼파라미터 설정\n",
    "alpha = 0.1  # 학습률: 새로운 정보가 오래된 정보를 얼마나 덮어쓰는지 결정\n",
    "gamma = 0.9  # 할인율: 미래 보상의 중요성을 결정\n",
    "epsilon = 0.1  # 탐험 비율: 무작위 행동을 선택할 확률 (탐험 vs. 활용)\n",
    "max_steps = 100  # 에피소드당 허용되는 최대 단계 수\n",
    "episodes = 500  # 실행할 총 에피소드 수\n",
    "\n",
    "# 모든 상태-행동 쌍에 대해 Q-테이블을 0으로 초기화\n",
    "q_table = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# 각 에피소드에서 누적된 총 보상을 저장할 리스트\n",
    "rewards_per_episode = []\n",
    "\n",
    "# 여러 에피소드의 Q-러닝 실행\n",
    "for episode in range(episodes):\n",
    "    # 단일 에피소드를 실행하고 총 보상 얻기\n",
    "    total_reward = run_episode(q_table, state_space, action_space, rewards, rows, cols, alpha, gamma, epsilon, max_steps)\n",
    "    # 이번 에피소드의 총 보상을 보상 리스트에 추가\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "# 가시성을 높이기 위해 그림 크기 조정\n",
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "# 에피소드에 걸쳐 누적된 총 보상 플로팅\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('에피소드')  # x축 레이블\n",
    "plt.ylabel('총 보상')  # y축 레이블\n",
    "plt.title('에피소드별 보상')  # 플롯 제목\n",
    "plt.show()  # 플롯 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**관찰된 학습 행동:**\n",
    "\n",
    "- **초기 에피소드**: 대부분 0 보상, 에이전트가 탐험 중임을 의미합니다.\n",
    "- **후기 에피소드**: 높은 보상 스파이크는 에이전트가 때때로 목표를 찾지만 일관성이 없음을 시사합니다.\n",
    "- **변동**: 정책이 아직 안정적이지 않음을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-테이블 초기화 및 업데이트\n",
    "경험을 바탕으로 Q-테이블을 초기화하고 업데이트하는 함수를 구현합니다. 이때 Q-러닝 업데이트 규칙을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블 초기화\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    \"\"\"\n",
    "    Q-테이블을 0으로 초기화합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - state_space: 가능한 모든 상태 목록.\n",
    "    - action_space: 가능한 행동 목록.\n",
    "    \n",
    "    반환값:\n",
    "    - q_table: 상태-행동 쌍을 Q-값에 매핑하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    q_table = {}\n",
    "    for state in state_space:\n",
    "        q_table[state] = {action: 0 for action in action_space}\n",
    "    return q_table\n",
    "\n",
    "# 엡실론 그리디 정책을 사용하여 행동 선택\n",
    "def choose_action(state, q_table, action_space, epsilon):\n",
    "    \"\"\"\n",
    "    엡실론 그리디 정책을 사용하여 행동을 선택합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - state: 현재 상태 (행, 열).\n",
    "    - q_table: 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - action_space: 가능한 행동 목록.\n",
    "    - epsilon: 탐험 비율.\n",
    "    \n",
    "    반환값:\n",
    "    - action: 선택된 행동.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(action_space)  # 탐험\n",
    "    else:\n",
    "        # q_table[state] 딕셔너리에서 값이 가장 큰 키(행동)를 반환\n",
    "        return max(q_table[state], key=q_table[state].get)  # 활용\n",
    "\n",
    "# Q-값 업데이트\n",
    "def update_q_value(q_table, state, action, reward, next_state, alpha, gamma, action_space):\n",
    "    \"\"\"\n",
    "    Q-러닝 업데이트 규칙을 사용하여 Q-값을 업데이트합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - q_table: 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - state: 현재 상태 (행, 열).\n",
    "    - action: 취한 행동.\n",
    "    - reward: 받은 보상.\n",
    "    - next_state: 다음 상태 (행, 열).\n",
    "    - alpha: 학습률.\n",
    "    - gamma: 할인율.\n",
    "    - action_space: 가능한 행동 목록.\n",
    "    \n",
    "    반환값:\n",
    "    - None (q_table을 제자리에서 업데이트).\n",
    "    \"\"\"\n",
    "    # 다음 상태에 대해 가능한 모든 행동 중 최대 Q-값을 가져옵니다.\n",
    "    # 다음 상태가 q_table에 있으면 해당 상태의 값들 중 최대값을, 없으면 0을 사용합니다.\n",
    "    max_next_q = max(q_table[next_state].values()) if next_state in q_table else 0\n",
    "    # 현재 상태-행동 쌍의 Q-값을 Q-러닝 공식으로 업데이트합니다.\n",
    "    q_table[state][action] += alpha * (reward + gamma * max_next_q - q_table[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 탐험 vs. 활용 전략\n",
    "이제 탐험 vs. 활용 전략을 제대로 사용하기 위해 이를 적절하게 구현해야 합니다. 첫 번째 함수는 엡실론 값에 따라 행동을 선택하는 엡실론 그리디 정책입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엡실론 그리디 정책 정의\n",
    "def epsilon_greedy_policy(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    state: Tuple[int, int], \n",
    "    action_space: List[str], \n",
    "    epsilon: float\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    행동 선택을 위한 엡실론 그리디 정책을 구현합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - q_table (Dict[Tuple[int, int], Dict[str, float]]): 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - state (Tuple[int, int]): 현재 상태 (행, 열).\n",
    "    - action_space (List[str]): 가능한 행동 목록.\n",
    "    - epsilon (float): 탐험 비율.\n",
    "    \n",
    "    반환값:\n",
    "    - str: 선택된 행동.\n",
    "    \"\"\"\n",
    "    # 확률 엡실론으로 무작위 행동 선택 (탐험)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(action_space)\n",
    "    # 그렇지 않으면 현재 상태에 대해 가장 높은 Q-값을 가진 행동 선택 (활용)\n",
    "    else:\n",
    "        # q_table[state] 딕셔너리에서 값이 가장 큰 키(행동)를 반환\n",
    "        return max(q_table[state], key=q_table[state].get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째 함수는 동적 엡실론 조정으로, 탐험과 활용의 균형을 맞추기 위해 시간이 지남에 따라 엡실론 값을 조정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 엡실론 조정 정의\n",
    "def adjust_epsilon(\n",
    "    initial_epsilon: float, \n",
    "    min_epsilon: float, \n",
    "    decay_rate: float, \n",
    "    episode: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    시간 경과에 따라 엡실론을 동적으로 조정하여 탐험과 활용의 균형을 맞춥니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - initial_epsilon (float): 초기 탐험 비율.\n",
    "    - min_epsilon (float): 최소 탐험 비율.\n",
    "    - decay_rate (float): 엡실론이 감소하는 비율.\n",
    "    - episode (int): 현재 에피소드 번호.\n",
    "    \n",
    "    반환값:\n",
    "    - float: 조정된 탐험 비율.\n",
    "    \"\"\"\n",
    "    # 감소된 엡실론 값을 계산하되, 최소 엡실론보다 작아지지 않도록 보장합니다.\n",
    "    return max(min_epsilon, initial_epsilon * np.exp(-decay_rate * episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로, 이 함수들을 사용하여 에피소드에 걸쳐 엡실론 값을 추적하고 감소를 플로팅할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엡실론 그리디 정책 및 동적 엡실론 조정 사용 예제\n",
    "initial_epsilon: float = 1.0  # 완전한 탐험으로 시작\n",
    "min_epsilon: float = 0.1  # 최소 탐험 비율\n",
    "decay_rate: float = 0.01  # 엡실론 감소율\n",
    "episodes: int = 500  # 에피소드 수\n",
    "\n",
    "# 에피소드에 걸쳐 엡실론 값 추적\n",
    "epsilon_values: List[float] = []\n",
    "for episode in range(episodes):\n",
    "    # 현재 에피소드에 대한 엡실론 조정\n",
    "    epsilon = adjust_epsilon(initial_epsilon, min_epsilon, decay_rate, episode)\n",
    "    epsilon_values.append(epsilon)\n",
    "\n",
    "# 가시성을 높이기 위해 그림 크기 조정\n",
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "# 에피소드에 걸쳐 엡실론 감소 플로팅\n",
    "plt.plot(epsilon_values)\n",
    "plt.xlabel('에피소드')  # x축 레이블\n",
    "plt.ylabel('엡실론')  # y축 레이블\n",
    "plt.title('에피소드별 엡실론 감소')  # 플롯 제목\n",
    "plt.show()  # 플롯 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **초기 탐험 (높은 엡실론 ~1.0)**  \n",
    "   - 첫 **100-150 에피소드** 동안, 에이전트는 높은 엡실론 때문에 **무작위**로 움직입니다.  \n",
    "   - **최적이 아닌 경로**를 포함하여 다양한 경로를 탐험하며, 이것이 보상 그래프에서 낮은 보상을 보인 이유입니다.  \n",
    "\n",
    "2. **중간 전환 (엡실론 감소 중)**  \n",
    "   - 에피소드 **150-250** 부근에서 엡실론이 감소하고, 에이전트는 여전히 탐험하면서 **더 나은 행동을 선호**하기 시작합니다.  \n",
    "   - 이는 보상 그래프에서 **높은 보상이 처음 나타나는 시점**과 일치하며, 에이전트가 **때때로 목표를 찾는다**는 의미입니다.  \n",
    "\n",
    "3. **후기 활용 (엡실론 안정화 ~0.1)**  \n",
    "   - **250+ 에피소드** 이후, 엡실론은 매우 낮아져 에이전트가 **대부분 가장 잘 알려진 경로를 따른다**는 의미입니다.  \n",
    "   - 보상 그래프는 **간헐적인 높은 보상**을 보여주며, 이는 에이전트가 **목표를 학습했지만 아직 100% 일관성이 없다**는 것을 나타냅니다.  \n",
    "\n",
    "**이것이 그리드 월드 훈련에 의미하는 바**  \n",
    "- **엡실론 감소는 에이전트 학습에 도움**을 주었습니다. 처음에는 광범위하게 탐험한 다음 전략을 개선했습니다.  \n",
    "- **불안정한 높은 보상**은 에이전트가 여전히 목표를 일관되게 찾는 데 어려움을 겪고 있음을 시사합니다.  \n",
    "- **가능한 해결책:** 학습 안정성을 개선하기 위해 **더 느린 엡실론 감소** 또는 **적응형 스케줄**을 시도해 보세요.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-러닝 알고리즘 실행\n",
    "그리드 월드 환경에서 Q-러닝 알고리즘을 실행하고 에이전트가 학습한 Q-값을 시각화해 보겠습니다. 각 상태-행동 쌍의 Q-값을 시간 경과에 따라 플로팅하여 훈련 중 어떻게 변화하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 에피소드에 걸쳐 Q-러닝 알고리즘을 실행하고 성능 지표 추적\n",
    "def run_q_learning(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    state_space: List[Tuple[int, int]], \n",
    "    action_space: List[str], \n",
    "    rewards: Dict[Tuple[int, int], int], \n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    alpha: float, \n",
    "    gamma: float, \n",
    "    initial_epsilon: float, \n",
    "    min_epsilon: float, \n",
    "    decay_rate: float, \n",
    "    episodes: int, \n",
    "    max_steps: int\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    여러 에피소드에 걸쳐 Q-러닝 알고리즘을 실행합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - q_table: 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - state_space: 가능한 모든 상태 목록.\n",
    "    - action_space: 가능한 행동 목록.\n",
    "    - rewards: (행, 열)을 보상 값에 매핑하는 딕셔너리.\n",
    "    - rows: 그리드의 행 수.\n",
    "    - cols: 그리드의 열 수.\n",
    "    - alpha: 학습률.\n",
    "    - gamma: 할인율.\n",
    "    - initial_epsilon: 초기 탐험 비율.\n",
    "    - min_epsilon: 최소 탐험 비율.\n",
    "    - decay_rate: 엡실론 감소율.\n",
    "    - episodes: 실행할 에피소드 수.\n",
    "    - max_steps: 에피소드당 최대 단계 수.\n",
    "    \n",
    "    반환값:\n",
    "    - rewards_per_episode: 에피소드당 총 보상 목록.\n",
    "    - episode_lengths: 에피소드 길이 목록.\n",
    "    \"\"\"\n",
    "    # 지표를 저장할 리스트 초기화\n",
    "    rewards_per_episode: List[int] = []\n",
    "    episode_lengths: List[int] = []\n",
    "    \n",
    "    # 각 에피소드를 순회\n",
    "    for episode in range(episodes):\n",
    "        # 무작위 상태에서 시작\n",
    "        state: Tuple[int, int] = state_space[np.random.choice(len(state_space))]\n",
    "        total_reward: int = 0  # 에피소드의 총 보상 초기화\n",
    "        steps: int = 0  # 단계 카운터 초기화\n",
    "        # 현재 에피소드에 대한 엡실론 조정\n",
    "        epsilon: float = adjust_epsilon(initial_epsilon, min_epsilon, decay_rate, episode)\n",
    "        \n",
    "        # 최대 단계 수만큼 반복\n",
    "        for _ in range(max_steps):\n",
    "            # 엡실론 그리디 정책을 사용하여 행동 선택\n",
    "            action: str = epsilon_greedy_policy(q_table, state, action_space, epsilon)\n",
    "            # 선택된 행동에 따라 다음 상태 계산\n",
    "            next_state: Tuple[int, int] = state_transition(state, action, rows, cols)\n",
    "            # 다음 상태에 대한 보상 얻기\n",
    "            reward: int = get_reward(next_state, rewards)\n",
    "            # 현재 상태-행동 쌍의 Q-값 업데이트\n",
    "            update_q_value(q_table, state, action, reward, next_state, alpha, gamma, action_space)\n",
    "            # 보상 누적\n",
    "            total_reward += reward\n",
    "            # 다음 상태로 이동\n",
    "            state = next_state\n",
    "            # 단계 카운터 증가\n",
    "            steps += 1\n",
    "            # 에이전트가 종료 상태에 도달했는지 확인\n",
    "            if state in terminal_states:\n",
    "                break\n",
    "        \n",
    "        # 현재 에피소드의 지표 추가\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    # 지표 반환\n",
    "    return rewards_per_episode, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 에피소드당 최대 `100` 단계로 `500` 에피소드에 걸쳐 Q-러닝 알고리즘을 실행합니다. 알고리즘을 실행하고 결과를 시각화해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-러닝 하이퍼파라미터 설정\n",
    "alpha: float = 0.1  # 학습률\n",
    "gamma: float = 0.9  # 할인율\n",
    "initial_epsilon: float = 1.0  # 초기 탐험 비율\n",
    "min_epsilon: float = 0.1  # 최소 탐험 비율\n",
    "decay_rate: float = 0.01  # 엡실론 감소율\n",
    "episodes: int = 500  # 에피소드 수\n",
    "max_steps: int = 100  # 에피소드당 최대 단계 수\n",
    "\n",
    "# Q-테이블 초기화\n",
    "q_table: Dict[Tuple[int, int], Dict[str, float]] = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# Q-러닝 알고리즘 실행\n",
    "rewards_per_episode, episode_lengths = run_q_learning(\n",
    "    q_table, state_space, action_space, rewards, rows, cols, alpha, gamma,\n",
    "    initial_epsilon, min_epsilon, decay_rate, episodes, max_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에피소드에 걸쳐 누적된 총 보상과 에피소드 길이를 플로팅하여 훈련 과정을 시각화해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에피소드에 걸쳐 누적 보상 플로팅\n",
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "# 에피소드당 총 보상 플로팅\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('에피소드')  # x축 레이블\n",
    "plt.ylabel('총 보상')  # y축 레이블\n",
    "plt.title('에피소드별 누적 보상')  # 플롯 제목\n",
    "\n",
    "# 에피소드당 에피소드 길이 플로팅\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(episode_lengths)\n",
    "plt.xlabel('에피소드')  # x축 레이블\n",
    "plt.ylabel('에피소드 길이')  # y축 레이블\n",
    "plt.title('에피소드별 길이')  # 플롯 제목\n",
    "\n",
    "# 레이아웃 조정 및 플롯 표시\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분석해 보겠습니다:\n",
    "\n",
    "**왼쪽 그래프: 에피소드별 누적 보상**\n",
    "- **에피소드당 총 보상**은 초기에 변동하며 성공과 실패가 혼재되어 있음을 보여줍니다.  \n",
    "- **후기 에피소드는 일관되게 최대 보상(10)을 달성**하여 에이전트가 최적 정책을 학습했음을 나타냅니다.  \n",
    "- 일부 낮은 보상 에피소드가 여전히 나타나며, 이는 가끔 발생하는 최적이 아닌 탐험 또는 확률적 행동을 시사합니다.  \n",
    "\n",
    "**그리드 월드와의 관계:**\n",
    "- 에이전트는 보상 10을 제공하는 **(3,3)으로 이동**하고 있습니다.\n",
    "- 처음에는 비효율적인 경로를 탐험하여 다양한 보상을 얻습니다.\n",
    "- 시간이 지남에 따라 **더 짧고 최적의 경로**를 찾습니다.\n",
    "\n",
    "**오른쪽 그래프: 에피소드별 에피소드 길이**\n",
    "- **초기 에피소드는 길이가 더 깁니다 (최대 50 단계).** 이는 에이전트가 비효율적인 경로를 택했음을 의미합니다.\n",
    "- 훈련이 진행됨에 따라 **에피소드 길이가 감소**하며, 이는 에이전트가 목표를 **더 빨리** 찾음을 나타냅니다.\n",
    "- **대부분의 에피소드는 낮은 단계 수 (~3-6 단계)에서 안정화**되며, 이는 에이전트가 경로를 최적화했음을 의미합니다.\n",
    "\n",
    "**그리드 월드와의 관계:**\n",
    "- 그리드 월드는 **4x4 (16 상태)에 불과**하므로 최적 정책은 빠르게 해결해야 합니다.\n",
    "- 처음에는 에이전트가 **무작위 또는 탐험적 이동**을 하여 단계 수가 증가했습니다.\n",
    "- Q-값이 수렴하면 에이전트는 **목표로 가는 직접적인 경로를 선택**하여 단계를 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 과정 시각화\n",
    "\n",
    "Q-테이블에서 실제로 무슨 일이 일어나고 있는지 시각화해야 합니다. 각 행동에 대한 Q-값을 히트맵으로, 학습된 정책을 그리드 위의 화살표로 시각화할 수 있습니다. 이를 돕기 위해 몇 가지 시각화 함수를 정의해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-값 히트맵 시각화 함수\n",
    "def plot_q_values(q_table: Dict[Tuple[int, int], Dict[str, float]], rows: int, cols: int, action_space: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    각 행동에 대한 Q-값을 히트맵으로 시각화합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - q_table (Dict[Tuple[int, int], Dict[str, float]]): 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - rows (int): 그리드의 행 수.\n",
    "    - cols (int): 그리드의 열 수.\n",
    "    - action_space (List[str]): 가능한 행동 목록.\n",
    "\n",
    "    반환값:\n",
    "    - None: 각 행동의 Q-값 히트맵을 표시합니다.\n",
    "    \"\"\"\n",
    "    # 각 행동에 대한 서브플롯 생성\n",
    "    fig, axes = plt.subplots(1, len(action_space), figsize=(15, 5))\n",
    "    for i, action in enumerate(action_space):\n",
    "        # 현재 행동에 대한 Q-값을 저장할 그리드 초기화\n",
    "        q_values = np.zeros((rows, cols))\n",
    "        for (row, col), actions in q_table.items():\n",
    "            q_values[row, col] = actions[action]  # 현재 행동에 대한 Q-값 추출\n",
    "\n",
    "        # 현재 행동에 대한 히트맵 플로팅\n",
    "        ax = axes[i]\n",
    "        cax = ax.matshow(q_values, cmap='viridis')\n",
    "        fig.colorbar(cax, ax=ax)\n",
    "        ax.set_title(f\"행동 '{action}'의 Q-값\")\n",
    "        ax.set_xlabel(\"열\")\n",
    "        ax.set_ylabel(\"행\")\n",
    "\n",
    "    # 레이아웃 조정 및 히트맵 표시\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 정책 시각화 함수\n",
    "def plot_policy(q_table: Dict[Tuple[int, int], Dict[str, float]], rows: int, cols: int) -> None:\n",
    "    \"\"\"\n",
    "    학습된 정책을 그리드 위의 화살표로 시각화합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - q_table (Dict[Tuple[int, int], Dict[str, float]]): 상태-행동 쌍을 Q-값에 매핑하는 Q-테이블.\n",
    "    - rows (int): 그리드의 행 수.\n",
    "    - cols (int): 그리드의 열 수.\n",
    "\n",
    "    반환값:\n",
    "    - None: 정책 시각화를 표시합니다.\n",
    "    \"\"\"\n",
    "    # 각 상태에 대한 최적 행동을 저장할 그리드 초기화\n",
    "    policy_grid = np.empty((rows, cols), dtype=str)\n",
    "    action_symbols = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}  # 행동 기호\n",
    "\n",
    "    # Q-값을 기반으로 각 상태에 대한 최적 행동 결정\n",
    "    for (row, col), actions in q_table.items():\n",
    "        best_action = max(actions, key=actions.get)  # 가장 높은 Q-값을 가진 행동 얻기\n",
    "        policy_grid[row, col] = action_symbols[best_action]  # 행동을 기호에 매핑\n",
    "\n",
    "    # 너비를 늘린 정책 그리드 플로팅\n",
    "    fig, ax = plt.subplots(figsize=(16, 3))  # 가로 스트레칭을 위해 너비를 12에서 16으로 늘림\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ax.text(j, i, policy_grid[i, j], ha='center', va='center', fontsize=14)  # 약간 더 큰 글꼴\n",
    "    \n",
    "    # 더 많은 가로 공간을 가진 더 넓은 그리드 생성\n",
    "    ax.set_xlim(-0.5, cols - 0.5)\n",
    "    ax.set_ylim(-0.5, rows - 0.5)\n",
    "    ax.matshow(np.zeros((rows, cols)), cmap='Greys', alpha=0.1)  # 희미한 배경 그리드 추가\n",
    "    ax.set_xticks(range(cols))\n",
    "    ax.set_yticks(range(rows))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(\"학습된 정책\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-값 히트맵과 학습된 정책을 나란히 플로팅\n",
    "fig, axes = plt.subplots(1, len(action_space) + 1, figsize=(20, 5))\n",
    "\n",
    "# 각 행동에 대한 Q-값 히트맵 플로팅\n",
    "for i, action in enumerate(action_space):\n",
    "    q_values = np.zeros((rows, cols))\n",
    "    for (row, col), actions in q_table.items():\n",
    "        q_values[row, col] = actions[action]\n",
    "    cax = axes[i].matshow(q_values, cmap='viridis')\n",
    "    fig.colorbar(cax, ax=axes[i])\n",
    "    axes[i].set_title(f\"행동 '{action}'의 Q-값\")\n",
    "    axes[i].set_xlabel(\"열\")\n",
    "    axes[i].set_ylabel(\"행\")\n",
    "\n",
    "# 학습된 정책 플로팅\n",
    "policy_grid = np.empty((rows, cols), dtype=str)\n",
    "action_symbols = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}\n",
    "for (row, col), actions in q_table.items():\n",
    "    best_action = max(actions, key=actions.get)\n",
    "    policy_grid[row, col] = action_symbols[best_action]\n",
    "\n",
    "axes[-1].matshow(np.zeros((rows, cols)), cmap='Greys', alpha=0.1)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        axes[-1].text(j, i, policy_grid[i, j], ha='center', va='center', fontsize=14)\n",
    "axes[-1].set_title(\"학습된 정책\")\n",
    "axes[-1].set_xlabel(\"열\")\n",
    "axes[-1].set_ylabel(\"행\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 시각화는 **4x4 그리드 월드**에 대한 **Q-값과 학습된 정책**을 제공합니다. 분석해 보겠습니다:\n",
    "\n",
    "**Q-값 히트맵 (왼쪽 네 개 플롯)**\n",
    "각 히트맵은 4x4 그리드의 각 상태에서 특정 행동 **(위, 아래, 왼쪽, 오른쪽)**에 대해 학습된 Q-값을 나타냅니다.\n",
    "\n",
    "- **더 높은 Q-값 (밝은 색상)은 해당 상태에서 선호되는 행동**을 나타냅니다.\n",
    "- **\"아래\" 및 \"오른쪽\" 행동이 가장 높은 Q-값**을 가지며, 이는 에이전트가 목표를 향해 이러한 방향으로 이동하는 것을 선호함을 의미합니다.\n",
    "- **오른쪽 하단 모서리 (목표 상태)는 \"위\" 및 \"오른쪽\"에서 매우 높은 값**을 가지며, 이는 에이전트가 이곳을 가치 있는 위치로 인식함을 시사합니다.\n",
    "\n",
    "**학습된 정책 (가장 오른쪽 플롯)**\n",
    "- **이는 학습된 Q-값을 기반으로 각 상태에서의 최적 행동**을 보여줍니다.\n",
    "- 에이전트는 일반적으로 **오른쪽 및 아래쪽 이동**을 따르며, 목표가 오른쪽 하단에 있다면 이는 합리적입니다.\n",
    "- 특정 위치에 **위쪽 화살표가 나타나는 것**은 장애물의 존재 또는 특정 이동에 대한 패널티를 시사합니다.\n",
    "\n",
    "**그리드 월드와의 비교**\n",
    "1. **에이전트는 대부분 효율적인 정책을 학습했습니다** (목표를 향해 오른쪽/아래로 이동).\n",
    "2. **일부 사소한 불일치** (몇몇 위/왼쪽 이동과 같은)는 다음으로 인해 발생할 수 있습니다:\n",
    "   - **여전히 탐험이 진행 중임** (ε-그리디 정책이 아직 완전히 탐욕적이지 않음).\n",
    "   - **학습률 또는 할인율이 가치 전파에 영향**을 미침.\n",
    "   - **가능한 장애물 또는 최적이 아닌 보상 구조**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-값 및 최적 정책 분석\n",
    "Q-러닝 알고리즘에 의해 학습된 최적 정책을 표 형식으로 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블 데이터를 나타내는 딕셔너리 리스트 생성\n",
    "q_policy_data = []\n",
    "for state, actions in q_table.items():\n",
    "    # 각 상태에 대해 모든 행동의 Q-값과 최적 행동을 포함하는 딕셔너리 추가\n",
    "    q_policy_data.append({\n",
    "        'State': state,  # 현재 상태 (행, 열)\n",
    "        'up': actions['up'],  # '위' 행동의 Q-값\n",
    "        'down': actions['down'],  # '아래' 행동의 Q-값\n",
    "        'left': actions['left'],  # '왼쪽' 행동의 Q-값\n",
    "        'right': actions['right'],  # '오른쪽' 행동의 Q-값\n",
    "        'Optimal Action': max(actions, key=actions.get)  # 가장 높은 Q-값을 가진 행동\n",
    "    })\n",
    "\n",
    "# Q-테이블 데이터를 표 형식으로 표시\n",
    "header = ['상태', '위', '아래', '왼쪽', '오른쪽', '최적 행동']  # 테이블 헤더 정의\n",
    "# 적절한 간격으로 테이블 헤더 출력\n",
    "print(f\"{header[0]:<10} {header[1]:<10} {header[2]:<10} {header[3]:<10} {header[4]:<10} {header[5]:<15}\")\n",
    "print(\"-\" * 65)  # 가독성을 위해 구분선 출력\n",
    "\n",
    "# Q-테이블 데이터를 반복하며 각 행 출력\n",
    "for row in q_policy_data:\n",
    "    # 상태, 모든 행동의 Q-값, 최적 행동 출력\n",
    "    print(f\"{str(row['State']):<10} {row['up']:<10.2f} {row['down']:<10.2f} {row['left']:<10.2f} {row['right']:<10.2f} {row['Optimal Action']:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주요 관찰 내용**\n",
    "1. **다른 행동 선호도**  \n",
    "   - 에이전트는 여전히 많은 경우 **오른쪽과 아래**로 이동하지만, 이전보다 **왼쪽과 위쪽 이동이 더 많아졌습니다**.  \n",
    "   - 특히, **상태 (0,0)은 \"위\"를 선호**하는 반면, (0,1)은 \"왼쪽\"을 선호하며, 이는 장애물이나 대체 최적 경로를 나타낼 수 있습니다.\n",
    "  \n",
    "2. **(3,3)은 가만히 있는 대신 \"위\"를 선호**  \n",
    "   - 만약 **(3,3)이 목표 상태**라면, 다른 행동에 대한 Q-값은 종료로 인해 **0**이거나 더 낮을 것으로 예상됩니다.\n",
    "   - 대신, \"위\"는 매우 높은 Q-값(19.23)을 가지며, 이는 에이전트가 멈추는 대신 여전히 이동을 고려하고 있음을 의미합니다.\n",
    "\n",
    "3. **전반적으로 더 높은 Q-값**\n",
    "   - **최대 Q-값이 증가**했습니다 (예: 상태 (2,3)은 \"아래\"에 대해 26.89의 값을 가짐).\n",
    "   - 이는 아마도 더 많은 훈련 에피소드나 다른 하이퍼파라미터(학습률, 할인율 등)로 인해 **더 많은 학습이 이루어졌음**을 시사합니다.\n",
    "\n",
    "4. **탐험 관련 잠재적 문제**\n",
    "   - (0,0)이 오른쪽 대신 \"위\"로 이동하는 것과 같이 일부 이동은 **덜 직관적으로 보입니다**.\n",
    "   - 이는 **탐험 비율(ε)이 여전히 높아서** 행동 선택에 약간의 무작위성이 발생하고 있음을 의미할 수 있습니다.\n",
    "\n",
    "**정책 시각화와의 비교**\n",
    "- 여기의 Q-값은 (이전에 공유한 히트맵에서) **정책 시각화 화살표**와 일치해야 합니다.  \n",
    "- 그러나 만약 **완벽하게 일치하지 않는다면**, 이는 다음을 의미할 수 있습니다:\n",
    "  - 진행 중인 학습으로 인해 **정책이 여전히 변동**하고 있습니다.\n",
    "  - Q-값에 영향을 미치는 **보상 불일치 또는 장애물**이 있습니다.\n",
    "  - 일부 **행동들의 Q-값이 유사**하여 동점 처리가 덜 명확합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 하이퍼파라미터로 테스트하기 (선택 사항)\n",
    "다양한 하이퍼파라미터를 실험하여 에이전트의 학습 과정에 어떤 영향을 미치는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 하이퍼파라미터로 실험\n",
    "learning_rates = [0.1, 0.5]  # 테스트할 다른 학습률 (alpha)\n",
    "discount_factors = [0.5]  # 테스트할 다른 할인율 (gamma) - 예시로 하나만 사용\n",
    "exploration_rates = [1.0]  # 테스트할 다른 초기 탐험 비율 (epsilon) - 예시로 하나만 사용\n",
    "\n",
    "# 비교를 위해 결과 저장\n",
    "results = []\n",
    "\n",
    "# 다른 하이퍼파라미터 조합으로 실험 실행\n",
    "for alpha in learning_rates:  # 다른 학습률 반복\n",
    "    for gamma in discount_factors:  # 다른 할인율 반복\n",
    "        for initial_epsilon in exploration_rates:  # 다른 초기 탐험 비율 반복\n",
    "            # 현재 실험을 위한 Q-테이블 초기화\n",
    "            q_table = initialize_q_table(state_space, action_space)\n",
    "            \n",
    "            # 현재 하이퍼파라미터 세트로 Q-러닝 실행\n",
    "            rewards_per_episode, episode_lengths = run_q_learning(\n",
    "                q_table, state_space, action_space, rewards, rows, cols, alpha, gamma,\n",
    "                initial_epsilon, min_epsilon, decay_rate, episodes, max_steps\n",
    "            )\n",
    "            \n",
    "            # 현재 실험 결과 저장\n",
    "            results.append({\n",
    "                'alpha': alpha,  # 학습률\n",
    "                'gamma': gamma,  # 할인율\n",
    "                'initial_epsilon': initial_epsilon,  # 초기 탐험 비율\n",
    "                'rewards_per_episode': rewards_per_episode,  # 에피소드당 수집된 보상\n",
    "                'episode_lengths': episode_lengths  # 각 에피소드의 길이\n",
    "            })\n",
    "\n",
    "# 모든 하이퍼파라미터 조합을 시각화하기 위해 더 큰 그림 생성\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# 서브플롯 그리드의 행과 열 수 계산\n",
    "num_rows = len(learning_rates)  # 행 수는 학습률의 개수와 같음\n",
    "num_cols = len(discount_factors) * len(exploration_rates)  # 열 수는 할인율과 탐험 비율 조합의 개수와 같음\n",
    "\n",
    "# 각 실험 결과 플로팅\n",
    "for i, result in enumerate(results):  # 모든 결과 반복\n",
    "    plt.subplot(num_rows, num_cols, i + 1)  # 각 실험에 대한 서브플롯 생성\n",
    "    plt.plot(result['rewards_per_episode'])  # 에피소드당 보상 플로팅\n",
    "    plt.title(f\"α={result['alpha']}, γ={result['gamma']}, ε={result['initial_epsilon']}\")  # 하이퍼파라미터 값으로 제목 추가\n",
    "    plt.xlabel('에피소드')  # x축 레이블\n",
    "    plt.ylabel('총 보상')  # y축 레이블\n",
    "\n",
    "# 겹침 방지 및 플롯 표시를 위한 레이아웃 조정\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 환경에 Q-러닝 적용하기 (절벽 걷기)\n",
    "환경을 절벽 걷기(Cliff Walking) 시나리오로 변경해 봅시다. 이 시나리오에서 에이전트는 목표 상태에 도달하기 위해 절벽이 있는 그리드를 탐색해야 합니다. 에이전트는 절벽에서 떨어지면 높은 음수 보상을 받고 목표 상태에 도달하면 양수 보상을 받습니다. 이 새로운 환경에 Q-러닝을 적용하고 결과를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절벽 걷기 환경 정의 \n",
    "def create_cliff_walking_env(\n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    cliff_states: List[Tuple[int, int]], \n",
    "    terminal_state: Tuple[int, int], \n",
    "    rewards: Dict[Tuple[int, int], int]\n",
    ") -> Tuple[np.ndarray, List[Tuple[int, int]], List[str]]:\n",
    "    \"\"\"\n",
    "    절벽 걷기 환경을 생성합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - rows (int): 그리드의 행 수.\n",
    "    - cols (int): 그리드의 열 수.\n",
    "    - cliff_states (List[Tuple[int, int]]): (행, 열) 튜플로 구성된 절벽 상태 목록.\n",
    "    - terminal_state (Tuple[int, int]): (행, 열) 튜플로 된 종료 상태.\n",
    "    - rewards (Dict[Tuple[int, int], int]): (행, 열)을 보상 값에 매핑하는 딕셔너리. (종료 상태 외에는 기본 이동 비용 -1 가정)\n",
    "\n",
    "    반환값:\n",
    "    - Tuple[np.ndarray, List[Tuple[int, int]], List[str]]:\n",
    "        - grid (np.ndarray): 보상을 포함한 그리드를 나타내는 2D 배열 (시각화용, 실제 보상 로직은 다름).\n",
    "        - state_space (List[Tuple[int, int]]): 그리드 내 가능한 모든 상태의 목록.\n",
    "        - action_space (List[str]): 가능한 행동 목록 ('up', 'down', 'left', 'right').\n",
    "    \"\"\"\n",
    "    # 시각화용 그리드 초기화 (실제 보상 계산과 다름)\n",
    "    grid = np.full((rows, cols), -1.0) # 기본 이동 비용 -1 로 시각화\n",
    "\n",
    "    # 지정된 종료 상태 보상 할당 (시각화용)\n",
    "    for (row, col), reward in rewards.items():\n",
    "        grid[row, col] = reward\n",
    "\n",
    "    # 절벽 상태에 높은 음수 보상 할당 (시각화용)\n",
    "    for row, col in cliff_states:\n",
    "        grid[row, col] = -100\n",
    "\n",
    "    # 상태 공간을 모든 가능한 (행, 열) 쌍으로 정의\n",
    "    state_space = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "\n",
    "    # 행동 공간을 네 가지 가능한 이동으로 정의\n",
    "    action_space = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    # 실제 보상 함수 (get_reward_cliff)는 별도로 정의해야 함\n",
    "    # 여기서는 시각화용 grid, state_space, action_space만 반환\n",
    "    return grid, state_space, action_space\n",
    "\n",
    "# 절벽 걷기 환경의 실제 보상 함수 정의\n",
    "def get_reward_cliff(state: Tuple[int, int], cliff_states: List[Tuple[int, int]], terminal_state: Tuple[int, int]) -> int:\n",
    "    \"\"\"\n",
    "    절벽 걷기 환경의 특정 상태에 대한 보상을 반환합니다.\n",
    "    절벽: -100, 목표: 0 (다음 상태가 목표일 때 +10은 update_q_value에서 처리), 그 외: -1\n",
    "    \"\"\"\n",
    "    if state in cliff_states:\n",
    "        return -100\n",
    "    elif state == terminal_state:\n",
    "        # 목표 상태 자체의 보상은 일반적으로 0으로 설정하고, \n",
    "        # 목표 상태로 '가는' 행동에 대한 보상을 긍정적으로 줌.\n",
    "        # 또는 update_q_value에서 r + gamma * max_q 부분을 사용.\n",
    "        # 여기서는 단순화를 위해 목표 도달 시 보상을 run_episode에서 처리하도록 하고 기본 이동 비용만 반환.\n",
    "        return -1 # 목표 상태에 도달해도 한 스텝 이동 비용은 발생\n",
    "    else:\n",
    "        return -1 # 일반적인 이동 비용\n",
    "\n",
    "# 절벽 걷기 환경의 상태 전이 함수 (경계 및 절벽 고려)\n",
    "def state_transition_cliff(state: Tuple[int, int], action: str, rows: int, cols: int, cliff_states: List[Tuple[int, int]], start_state: Tuple[int, int]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    절벽 걷기 환경에서 상태 전이를 계산합니다. 절벽에 빠지면 시작 지점으로 돌아갑니다.\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    next_row, next_col = row, col\n",
    "\n",
    "    if action == 'up':\n",
    "        next_row = max(0, row - 1)\n",
    "    elif action == 'down':\n",
    "        next_row = min(rows - 1, row + 1)\n",
    "    elif action == 'left':\n",
    "        next_col = max(0, col - 1)\n",
    "    elif action == 'right':\n",
    "        next_col = min(cols - 1, col + 1)\n",
    "\n",
    "    next_state = (next_row, next_col)\n",
    "\n",
    "    # 절벽에 빠졌는지 확인\n",
    "    if next_state in cliff_states:\n",
    "        return start_state # 절벽에 빠지면 시작 상태로 돌아감\n",
    "    else:\n",
    "        return next_state\n",
    "\n",
    "# Q-값 업데이트 함수 (절벽 환경용, 실제 보상 함수 사용)\n",
    "def update_q_value_cliff(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    state: Tuple[int, int], \n",
    "    action: str, \n",
    "    reward: int, # get_reward_cliff에서 받은 보상 (-1 또는 -100)\n",
    "    next_state: Tuple[int, int], \n",
    "    alpha: float, \n",
    "    gamma: float\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    절벽 걷기 환경에 맞게 Q-값을 업데이트합니다.\n",
    "    \"\"\"\n",
    "    # 다음 상태가 종료 상태인지 확인 (여기서는 절벽에 빠지는 것도 종료 조건일 수 있음)\n",
    "    # 다음 상태의 최대 Q값 계산\n",
    "    max_next_q: float = max(q_table[next_state].values()) if next_state in q_table else 0.0\n",
    "\n",
    "    # Q-값 업데이트\n",
    "    q_table[state][action] += alpha * (reward + gamma * max_next_q - q_table[state][action])\n",
    "\n",
    "# 에피소드 실행 함수 (절벽 환경용)\n",
    "def run_episode_cliff(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    state_space: List[Tuple[int, int]], \n",
    "    action_space: List[str], \n",
    "    cliff_states: List[Tuple[int, int]],\n",
    "    terminal_state: Tuple[int, int],\n",
    "    start_state: Tuple[int, int],\n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    alpha: float, \n",
    "    gamma: float, \n",
    "    epsilon: float, \n",
    "    max_steps: int\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    절벽 걷기 환경에서 단일 Q-러닝 에피소드를 실행합니다.\n",
    "    \"\"\"\n",
    "    state = start_state\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while state != terminal_state and steps < max_steps:\n",
    "        action = choose_action(state, q_table, action_space, epsilon)\n",
    "        next_state = state_transition_cliff(state, action, rows, cols, cliff_states, start_state)\n",
    "        reward = get_reward_cliff(next_state, cliff_states, terminal_state) # 다음 상태 기준 보상\n",
    "        \n",
    "        update_q_value_cliff(q_table, state, action, reward, next_state, alpha, gamma)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        # 절벽에 빠졌을 경우 에피소드 종료 (시작 지점으로 돌아가므로)\n",
    "        # 이 부분은 구현에 따라 다를 수 있음. 여기서는 max_steps까지 계속 진행.\n",
    "        # if next_state == start_state and state != start_state: # 절벽에 빠져 돌아온 경우\n",
    "            # break # 에피소드를 여기서 끝낼 수도 있음\n",
    "            \n",
    "    return total_reward\n",
    "\n",
    "# Q-러닝 실행 함수 (절벽 환경용)\n",
    "def run_q_learning_cliff(\n",
    "    state_space: List[Tuple[int, int]], \n",
    "    action_space: List[str], \n",
    "    cliff_states: List[Tuple[int, int]], \n",
    "    terminal_state: Tuple[int, int], \n",
    "    start_state: Tuple[int, int], \n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    alpha: float, \n",
    "    gamma: float, \n",
    "    initial_epsilon: float, \n",
    "    min_epsilon: float, \n",
    "    decay_rate: float, \n",
    "    episodes: int, \n",
    "    max_steps: int\n",
    ") -> Tuple[Dict[Tuple[int, int], Dict[str, float]], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    절벽 걷기 환경에서 여러 에피소드에 걸쳐 Q-러닝을 실행합니다.\n",
    "    \"\"\"\n",
    "    q_table = initialize_q_table(state_space, action_space)\n",
    "    rewards_per_episode = []\n",
    "    # episode_lengths = [] # 필요하다면 길이 추적\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        epsilon = adjust_epsilon(initial_epsilon, min_epsilon, decay_rate, episode)\n",
    "        total_reward = run_episode_cliff(\n",
    "            q_table, state_space, action_space, cliff_states, terminal_state, start_state,\n",
    "            rows, cols, alpha, gamma, epsilon, max_steps\n",
    "        )\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        # episode_lengths.append(steps) # run_episode_cliff에서 steps 반환 필요\n",
    "        \n",
    "        # 진행 상황 출력 (선택 사항)\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"에피소드 {episode + 1}/{episodes} 완료, 평균 보상 (최근 100): {np.mean(rewards_per_episode[-100:]):.2f}\")\n",
    "\n",
    "    # 여기서는 episode_lengths를 반환하지 않음 (필요 시 추가)\n",
    "    return q_table, rewards_per_episode, [] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "절벽 걷기 환경을 생성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절벽 걷기 환경 정의\n",
    "rows, cols = 4, 12  # 그리드 차원 (4행 12열)\n",
    "start_state = (3, 0) # 시작 위치\n",
    "\n",
    "# 절벽 상태 정의 (맨 아래 행, 시작과 목표 제외)\n",
    "cliff_states = [(3, c) for c in range(1, 11)]  \n",
    "\n",
    "# 종료 상태 (목표 위치) 정의\n",
    "terminal_state = (3, 11)\n",
    "\n",
    "# 환경 보상 정의 (실제 보상 로직은 get_reward_cliff 사용)\n",
    "# create_cliff_walking_env 에는 종료 상태 보상만 전달 (시각화용)\n",
    "cliff_rewards_viz = {terminal_state: 0} # 목표 자체 보상은 0, 도달 시 +10 효과는 업데이트 규칙에 반영됨\n",
    "\n",
    "# 절벽 걷기 환경 생성 (시각화용 그리드, 상태 공간, 행동 공간)\n",
    "cliff_grid_viz, cliff_state_space, cliff_action_space = create_cliff_walking_env(\n",
    "    rows, cols, cliff_states, terminal_state, cliff_rewards_viz\n",
    ")\n",
    "\n",
    "# 시작 상태 시각화용 그리드 값 설정 (선택 사항)\n",
    "# cliff_grid_viz[start_state] = -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보상을 플로팅하기 위해 다음 코드를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절벽 걷기 환경의 보상 플로팅 함수\n",
    "def plot_rewards(rewards_per_episode: List[int], ax: plt.Axes = None) -> plt.Axes:\n",
    "    \"\"\"\n",
    "    에피소드에 걸쳐 누적된 총 보상을 플로팅합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - rewards_per_episode (List[int]): 에피소드당 총 보상 목록.\n",
    "    - ax (plt.Axes, optional): 플로팅할 Matplotlib 축. None이면 새 그림과 축을 생성합니다.\n",
    "\n",
    "    반환값:\n",
    "    - plt.Axes: 플롯이 포함된 Matplotlib 축.\n",
    "    \"\"\"\n",
    "    # 축이 제공되지 않으면 새 그림과 축 생성\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # 에피소드별 보상 플로팅\n",
    "    ax.plot(rewards_per_episode)\n",
    "    ax.set_xlabel('에피소드')  # x축 레이블\n",
    "    ax.set_ylabel('총 보상')  # y축 레이블\n",
    "    ax.set_title('에피소드별 보상')  # 플롯 제목\n",
    "    \n",
    "    # 추가 사용자 정의를 위해 축 반환\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "절벽 걷기 환경을 플로팅하고 학습된 정책을 시각화하는 몇 가지 함수를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절벽 걷기 환경 시각화\n",
    "def plot_cliff_walking_env(\n",
    "    grid: np.ndarray, \n",
    "    cliff_states: List[Tuple[int, int]], \n",
    "    terminal_state: Tuple[int, int], \n",
    "    start_state: Tuple[int, int], # 시작 상태 추가\n",
    "    ax: Optional[plt.Axes] = None\n",
    ") -> plt.Axes:\n",
    "    \"\"\"\n",
    "    절벽 걷기 환경을 시각화합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - grid: 그리드를 나타내는 2D numpy 배열.\n",
    "    - cliff_states: (행, 열) 튜플로 구성된 절벽 상태 목록.\n",
    "    - terminal_state: (행, 열) 튜플로 된 종료 상태.\n",
    "    - start_state: (행, 열) 튜플로 된 시작 상태.\n",
    "    - ax: 플로팅할 선택적 Matplotlib 축.\n",
    "\n",
    "    반환값:\n",
    "    - ax: 환경 시각화가 포함된 Matplotlib 축.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4)) # 가로로 길게 조정\n",
    "        \n",
    "    rows, cols = grid.shape\n",
    "    # 배경 색상 설정 (흰색)\n",
    "    ax.imshow(np.ones((rows, cols)), cmap='Greys', alpha=0.1)\n",
    "        \n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if (r, c) in cliff_states:\n",
    "                # 절벽 영역 채우기\n",
    "                ax.add_patch(plt.Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor='lightcoral', edgecolor='black'))\n",
    "                ax.text(c, r, 'C', ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "            elif (r, c) == terminal_state:\n",
    "                # 목표 지점\n",
    "                ax.add_patch(plt.Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor='lightgreen', edgecolor='black'))\n",
    "                ax.text(c, r, 'G', ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "            elif (r, c) == start_state:\n",
    "                # 시작 지점\n",
    "                ax.add_patch(plt.Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor='lightblue', edgecolor='black'))\n",
    "                ax.text(c, r, 'S', ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "            else:\n",
    "                 # 일반 칸\n",
    "                 ax.add_patch(plt.Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor='white', edgecolor='lightgray'))\n",
    "                 # ax.text(c, r, '.', ha='center', va='center', color='gray', fontsize=10) # 점 제거\n",
    "    \n",
    "    # 축 설정\n",
    "    ax.set_xticks(np.arange(-0.5, cols, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, rows, 1), minor=True)\n",
    "    ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "    ax.tick_params(which='minor', size=0)\n",
    "    ax.set_xticks(np.arange(0, cols, 1))\n",
    "    ax.set_yticks(np.arange(0, rows, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(\"절벽 걷기 환경\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 행동에 대한 Q-값 시각화\n",
    "# 참고: Cliff Walking에서는 상태 가치(V) 또는 최대 Q값을 시각화하는 것이 더 일반적임\n",
    "def plot_q_values_cliff(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    cliff_states: List[Tuple[int, int]], # 절벽 상태 정보 추가\n",
    "    terminal_state: Tuple[int, int], # 종료 상태 정보 추가\n",
    "    ax: Optional[plt.Axes] = None\n",
    ") -> plt.Axes:\n",
    "    \"\"\"\n",
    "    각 상태의 최대 Q-값을 시각화합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - q_table: 각 상태-행동 쌍에 대한 Q-값을 포함하는 딕셔너리.\n",
    "    - rows: 그리드의 행 수.\n",
    "    - cols: 그리드의 열 수.\n",
    "    - cliff_states: 절벽 상태 목록.\n",
    "    - terminal_state: 종료 상태.\n",
    "    - ax: 플로팅할 선택적 Matplotlib 축.\n",
    "\n",
    "    반환값:\n",
    "    - ax: Q-값 시각화가 포함된 Matplotlib 축.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4)) # 가로로 길게 조정\n",
    "    \n",
    "    # Q-값을 위한 그리드 생성\n",
    "    max_q_values = np.full((rows, cols), -np.inf) # 초기값을 음의 무한대로 설정\n",
    "    \n",
    "    # 각 상태에 대해 가장 높은 Q-값을 가진 행동 찾기\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            state = (r, c)\n",
    "            if state in q_table:\n",
    "                max_q_values[r, c] = max(q_table[state].values())\n",
    "            # Q-테이블에 없는 상태 (예: 도달 불가능)는 -inf 유지\n",
    "\n",
    "    # 절벽과 목표 상태는 Q값이 의미 없을 수 있으므로 마스킹 (NaN으로)\n",
    "    for r, c in cliff_states:\n",
    "        max_q_values[r, c] = np.nan\n",
    "    max_q_values[terminal_state] = np.nan\n",
    "\n",
    "    # 결측값 제외하고 최소/최대값 찾기\n",
    "    q_min = np.nanmin(max_q_values)\n",
    "    q_max = np.nanmax(max_q_values)\n",
    "    \n",
    "    im = ax.imshow(max_q_values, cmap='viridis', vmin=q_min, vmax=q_max)\n",
    "    plt.colorbar(im, ax=ax, label='최대 Q-값')\n",
    "    \n",
    "    # 상태 값을 텍스트로 추가\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if not np.isnan(max_q_values[r, c]):\n",
    "                # 텍스트 색상 결정 (배경 밝기에 따라)\n",
    "                # 정규화된 값 계산\n",
    "                normalized_q = (max_q_values[r,c] - q_min) / (q_max - q_min) if (q_max - q_min) != 0 else 0.5\n",
    "                text_color = 'white' if normalized_q < 0.5 else 'black'\n",
    "                ax.text(c, r, f\"{max_q_values[r, c]:.1f}\", ha='center', va='center', color=text_color)\n",
    "            elif (r, c) in cliff_states:\n",
    "                 ax.text(c, r, 'C', ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "            elif (r, c) == terminal_state:\n",
    "                 ax.text(c, r, 'G', ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "            elif (r,c) == start_state:\n",
    "                 ax.text(c, r, 'S', ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "            \n",
    "\n",
    "    ax.set_xticks(range(cols))\n",
    "    ax.set_yticks(range(rows))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title('상태별 최대 Q-값')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블에서 파생된 정책 시각화\n",
    "def plot_policy_cliff(\n",
    "    q_table: Dict[Tuple[int, int], Dict[str, float]], \n",
    "    rows: int, \n",
    "    cols: int, \n",
    "    cliff_states: List[Tuple[int, int]], # 절벽 상태 정보 추가\n",
    "    terminal_state: Tuple[int, int], # 종료 상태 정보 추가\n",
    "    ax: Optional[plt.Axes] = None\n",
    ") -> plt.Axes:\n",
    "    \"\"\"\n",
    "    Q-테이블에서 파생된 정책을 시각화합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - q_table: 각 상태-행동 쌍에 대한 Q-값을 포함하는 딕셔너리.\n",
    "    - rows: 그리드의 행 수.\n",
    "    - cols: 그리드의 열 수.\n",
    "    - cliff_states: 절벽 상태 목록.\n",
    "    - terminal_state: 종료 상태.\n",
    "    - ax: 플로팅할 선택적 Matplotlib 축.\n",
    "\n",
    "    반환값:\n",
    "    - ax: 정책 시각화가 포함된 Matplotlib 축.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4)) # 가로로 길게 조정\n",
    "    \n",
    "    # 행동 기호 정의\n",
    "    action_symbols = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}\n",
    "    \n",
    "    # 정책을 위한 그리드 생성\n",
    "    policy_grid = np.empty((rows, cols), dtype='U1')\n",
    "    \n",
    "    # 각 상태에 대해 가장 높은 Q-값을 가진 행동 찾기\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            state = (r, c)\n",
    "            # 절벽이나 목표 상태가 아닌 경우에만 화살표 표시\n",
    "            if state in q_table and state not in cliff_states and state != terminal_state:\n",
    "                # 모든 Q값이 같은 경우 (예: 초기 상태) 처리\n",
    "                q_values = q_table[state]\n",
    "                if len(set(q_values.values())) == 1:\n",
    "                     policy_grid[r, c] = '·' # 동점일 경우 점으로 표시 (또는 무작위 선택)\n",
    "                else:\n",
    "                     best_action = max(q_values, key=q_values.get)\n",
    "                     policy_grid[r, c] = action_symbols[best_action]\n",
    "            elif state in cliff_states:\n",
    "                policy_grid[r, c] = 'C' # 절벽\n",
    "            elif state == terminal_state:\n",
    "                 policy_grid[r, c] = 'G' # 목표\n",
    "            elif state == start_state:\n",
    "                 policy_grid[r, c] = 'S' # 시작\n",
    "            else:\n",
    "                policy_grid[r, c] = ' ' # Q값이 없는 상태\n",
    "    \n",
    "    # 정책 그리드 표시 (배경)\n",
    "    ax.imshow(np.zeros((rows, cols)), cmap='Greys', alpha=0.1)\n",
    "    \n",
    "    # 정책 화살표를 텍스트로 추가\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            text_color = 'black' # 기본 텍스트 색상\n",
    "            font_size = 15\n",
    "            if policy_grid[r, c] in ['C', 'G', 'S']:\n",
    "                 font_size = 12\n",
    "                 text_color = 'dimgray'\n",
    "            if policy_grid[r, c] != ' ':\n",
    "                 ax.text(c, r, policy_grid[r, c], ha='center', va='center', color=text_color, fontsize=font_size)\n",
    "    \n",
    "    ax.set_xticks(range(cols))\n",
    "    ax.set_yticks(range(rows))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title('학습된 정책')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절벽 걷기 환경에서 Q-러닝 실행\n",
    "alpha = 0.1  # 학습률\n",
    "gamma = 0.9  # 할인율\n",
    "initial_epsilon = 1.0  # 초기 탐험 비율\n",
    "min_epsilon = 0.1  # 최소 탐험 비율\n",
    "decay_rate = 0.005 # 엡실론 감소율 (조정됨)\n",
    "episodes = 1000  # 에피소드 수 (증가됨)\n",
    "max_steps = 200  # 에피소드당 최대 단계 수 (증가됨)\n",
    "\n",
    "# Q-러닝 실행 (절벽 환경용 함수 사용)\n",
    "cliff_q_table, cliff_rewards_per_episode, _ = run_q_learning_cliff(\n",
    "    cliff_state_space, cliff_action_space, cliff_states, terminal_state, start_state, \n",
    "    rows, cols, alpha, gamma, initial_epsilon, min_epsilon, decay_rate, episodes, max_steps\n",
    ")\n",
    "\n",
    "# 시각화를 위한 2x2 그리드 생성\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 9)) # 크기 조정\n",
    "\n",
    "# 왼쪽 상단 서브플롯에 보상 플로팅\n",
    "plot_rewards(cliff_rewards_per_episode, ax=axs[0, 0])\n",
    "\n",
    "# 오른쪽 상단 서브플롯에 환경 플로팅\n",
    "plot_cliff_walking_env(cliff_grid_viz, cliff_states, terminal_state, start_state, ax=axs[0, 1])\n",
    "\n",
    "# 왼쪽 하단 서브플롯에 Q-값 플로팅\n",
    "plot_q_values_cliff(cliff_q_table, rows, cols, cliff_states, terminal_state, ax=axs[1, 0])\n",
    "\n",
    "# 오른쪽 하단 서브플롯에 정책 플로팅\n",
    "plot_policy_cliff(cliff_q_table, rows, cols, cliff_states, terminal_state, ax=axs[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주요 관찰 내용:**\n",
    "1. **왼쪽 상단: 에피소드별 보상**  \n",
    "   - 총 보상이 **점진적으로 증가**하며, 이는 에이전트가 시간이 지남에 따라 더 나은 정책을 학습하고 있음을 나타냅니다.  \n",
    "   - **초기에 큰 폭의 음수 보상**이 관찰되며, 이는 에이전트가 절벽에 자주 빠졌음을 의미합니다. 학습이 진행됨에 따라 보상이 덜 부정적으로 변합니다. \n",
    "   - 후반부에도 보상이 약간 변동하는 것은 탐험(엡실론) 또는 환경의 확률적 요소 때문일 수 있습니다.\n",
    "\n",
    "2. **오른쪽 상단: 절벽 걷기 그리드 표현**  \n",
    "   - 환경은 **4행 12열 그리드**로 보입니다.  \n",
    "   - **시작 지점(S)**은 왼쪽 하단에 있습니다.  \n",
    "   - **목표 지점(G)**은 오른쪽 하단에 있습니다.  \n",
    "   - 빨간색으로 표시된 **절벽(C)**은 맨 아래 행을 따라 있으며, 에이전트가 이곳을 밟으면 큰 페널티(-100)를 받고 시작 지점으로 돌아갑니다.\n",
    "\n",
    "3. **왼쪽 하단: 상태별 최대 Q-값**  \n",
    "   - 이 히트맵은 각 상태의 **가장 높은 Q-값**을 나타냅니다.  \n",
    "   - 예상대로 **목표 지점(G)에 가까워질수록 값이 증가**합니다.  \n",
    "   - **절벽 바로 위 행(행 2)**의 값들이 상대적으로 높게 나타나며, 이는 절벽을 피해가는 안전한 경로의 가치를 반영합니다. 절벽 지역 자체는 시각화에서 제외되었습니다.\n",
    "\n",
    "4. **오른쪽 하단: 학습된 정책**  \n",
    "   - 이는 화살표를 사용하여 **상태별 최적 행동**을 보여줍니다.  \n",
    "   - 대부분의 화살표는 **오른쪽(→)**을 가리키며 목표를 향해 에이전트를 안내합니다.  \n",
    "   - 특히 **절벽 바로 위(행 2)**에서는 에이전트가 오른쪽으로 이동하는 안전한 경로를 명확히 학습했음을 보여줍니다. \n",
    "   - 시작 지점 근처에서는 위(↑)로 이동하여 절벽을 피하려는 경향을 보입니다.\n",
    "\n",
    "**분석:**\n",
    "- **정책이 잘 학습**되었으며, 에이전트는 대부분 절벽을 피합니다.\n",
    "- **초기 탐험으로 인해 초기 실패**가 발생했으며, 이는 보상 그래프의 급격한 하락에서 볼 수 있습니다.\n",
    "- 이 환경을 해결하기 위해 **Q-러닝 또는 SARSA**를 사용했을 수 있습니다. \n",
    "  - **Q-러닝**이라면, 최적의 장기 전략(때로는 위험 감수)을 학습합니다.\n",
    "  - **SARSA**라면, 현재 정책을 따르면서 더 안전한 정책(위험한 단계를 피함)을 학습하는 경향이 있습니다. (이 노트북은 Q-러닝을 사용했습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반적인 문제점 및 해결책\n",
    "\n",
    "**문제점: 큰 상태 공간에서의 느린 학습**\n",
    "\n",
    "**해결책**: 이를 처리하는 몇 가지 접근 방식이 있습니다:\n",
    "- 함수 근사 (신경망을 사용하여 Q-값 근사)\n",
    "- 상태 집계 (유사한 상태 그룹화)\n",
    "- 경험 리플레이 (과거 경험 재사용)\n",
    "\n",
    "**문제점: 탐험과 활용의 균형 맞추기**\n",
    "\n",
    "**해결책**: \n",
    "- 높은 탐험으로 시작 (높은 ε)\n",
    "- 시간이 지남에 따라 점진적으로 탐험 감소 (ε 감소)\n",
    "- 볼츠만 탐험과 같은 더 정교한 탐험 전략 사용\n",
    "\n",
    "**문제점: 적절한 하이퍼파라미터 선택**\n",
    "\n",
    "**해결책**:\n",
    "- 학습률 (α): 0.1-0.3으로 시작\n",
    "- 할인율 (γ): 일반적으로 0.9-0.99\n",
    "- 탐험 비율 (ε): 높게 시작(0.9-1.0)하고 낮은 값(0.01-0.1)으로 감소\n",
    "- 최적 설정을 찾기 위해 체계적으로 다른 조합 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-러닝 vs. 다른 강화 학습 알고리즘\n",
    "\n",
    "### Q-러닝의 장점\n",
    "- 이해하고 구현하기 간단함\n",
    "- 모델-프리 (환경 역학을 알 필요 없음)\n",
    "- 최적 정책을 직접 학습할 수 있음\n",
    "- 이산적인 상태 및 행동 공간에서 잘 작동함\n",
    "\n",
    "### Q-러닝의 한계\n",
    "- 크거나 연속적인 상태 공간에서 어려움을 겪음\n",
    "- 수렴 속도가 느릴 수 있음\n",
    "- Q-값을 과대평가할 수 있음 (Double Q-Learning으로 해결)\n",
    "- 연속적인 행동 공간에 직접 적용하기 어려움\n",
    "\n",
    "### 관련 알고리즘\n",
    "- **SARSA**: Q-러닝과 유사하지만 최대 Q-값 대신 실제 다음 행동을 사용\n",
    "- **Deep Q-Network (DQN)**: 큰 상태 공간에 대해 Q-값을 근사하기 위해 신경망 사용\n",
    "- **Double Q-Learning**: Q-러닝의 과대평가 문제 해결\n",
    "- **함수 근사를 사용한 Q-러닝**: 크거나 연속적인 상태 공간에 함수 근사 사용\n",
    "\n",
    "## 결론\n",
    "\n",
    "Q-러닝은 많은 문제에 성공적으로 적용된 강력하고 직관적인 강화 학습 알고리즘입니다. 그 강점은 단순성과 환경 모델 없이 학습할 수 있는 능력에 있습니다. 매우 큰 상태 공간에 대한 한계가 있지만, Deep Q-Networks와 같은 확장을 통해 이러한 문제 중 다수가 해결되었습니다.\n",
    "\n",
    "Q-러닝을 이해하는 것은 더 발전된 강화 학습 알고리즘과 개념을 탐구하기 위한 견고한 기반을 제공합니다. Q-러닝의 핵심 아이디어—경험을 통해 상태-행동 쌍의 가치를 학습하고 이 가치를 사용하여 결정을 내리는 것—는 현대 강화 학습 전반에 걸쳐 나타납니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
